# Deep-Researcher: Agentic RAG Excellence

Deep-Researcher is a high-performance, agentic RAG (Retrieval-Augmented Generation) system designed for deep intelligence gathering across multiple data sources. It leverages advanced orchestration frameworks and a multimodal brain to plan, execute, and synthesize complex research tasks.

## ğŸ—ï¸ System Architecture

The project follows a **layered microservice architecture** with clear separation between frontend, API gateway, orchestration layer, and storage.

### 1. Interface Tier (Frontend UI)
- **Desktop Application**: Built with **Vite + React** (using shadcn/ui components).
- **Real-time Collaboration**: Frontend polling for agent progress and streaming outputs.

### 2. API Gateway (BFF - Backend for Frontend)
- **Framework**: Python **FastAPI** with **Gevent** workers.
- **Responsibilities**:
  - API endpoint management
  - Ability to handle file uploads (PDF, DOCX, TXT)
  - Real-time progress updates
  - Streaming research outputs
  - Export API (PDF, Word, CSV, Excel)

### 3. Orchestration & Agent Layer
- **Frameworks**: **LangChain**, **LlamaIndex**, and **LangFlow** for visual agent workflows.
- **User Config Manager**: Handles RSPY Toggle, Source Selection, Depth Level, Report Preferences.
- **Background Task Queue**: Async task execution with progress tracking.

#### Specialized Agents
- **Web Research Agent**: Uses **Crawl4AI** (DuckDuckGo) for autonomous web crawling.
- **YouTube Research Agent**: Extracts and transcribes video content using **Python API**.
- **Document Retrieval Agent**: Parses PDFs, DOCX, and TXT files.
- **Image Metadata Agent**: Analyzes visual content for additional context.

### 4. Data Ingestion & Processing
- **Trace Email Handler**: Ingests email threads for research.
- **Document Parser**: Multi-format support (PDF, DOCX, TXT).
- **Audio Transcriber**: **Whisper** integration for voice/video transcription.
- **Realtime Research Server**: High-end GPU/Laptop server for real-time inference.

### 5. The Brain (LLMs)
- **Generative Agent**: **Ollama** (Local LLM) for reasoning and synthesis.
- **Cloud Fallback**: **OpenAI GPT-4**, **Google Gemini**, **Anthropic Claude**.

### 6. Storage & Persistence Layer

#### SQLite Databases
- **brain.db**: Core knowledge storage.
- **history.db**: User interaction logs and session history.
- **scrapes.db**: Cached web scrapes and crawl data.
- **research.db**: Completed research tasks and reports.
- **assets.db**: Metadata for images, files, and attachments.
- **export.db**: Export job tracking and archived outputs.

#### Vector & Memory Stores
- **Vector Store**: **ChromaDB** for semantic search and RAG retrieval.
- **Raw Data Store**: **File System** for unprocessed documents and media.
- **Metadata Store**: **SQLite** for structured metadata.

#### Context Management
- **Context Builder**: Constructs prompts from vector store + raw data.
- **LlamaIndex**: Handles semantic chunking and embedding generation.

### 7. Export & Output System
- **Research Document Generator**: Produces structured reports.
- **Export Formats**: PDF, Word (`.docx`), Plain Text, CSV, Excel.
- **Research History Manager**: Tracks previous queries and generated outputs.
- **Session Logs**: Full audit trail of agent reasoning and tool use.

---

## ğŸ”„ Agentic Reasoning Flow

1. **User Query Analysis**: Parse intent, configure RSPY (Research, Synthesis, Planning, YouTube) toggles.
2. **Unified Research Request**: Route to appropriate agents based on config.
3. **Parallel Agent Execution**:
   - Web Research Agent â†’ Crawl4AI â†’ Scrapes DB
   - YouTube Agent â†’ Transcription â†’ Assets DB
   - Document Agent â†’ Parser â†’ Research DB
   - Image Agent â†’ Metadata Extraction
4. **Data Annotation & Vectorization**: Chunk, embed, and store in ChromaDB.
5. **Context Building**: Retrieve top-K relevant chunks + metadata.
6. **LLM Synthesis**: Generate coherent research report with citations.
7. **Export & Storage**: Save to Research DB, offer multi-format export.

---

## ğŸ› ï¸ Implementation Roadmap

### Phase 1: Backend & Storage Foundation
- [ ] Set up FastAPI with BFF pattern and Gevent workers.
- [ ] Initialize **6 SQLite databases** (brain, history, scrapes, research, assets, export).
- [ ] Configure **ChromaDB** for vector storage.
- [ ] Set up file system storage for raw data.
- [ ] Integrate **Ollama** for local LLM inference.

### Phase 2: Agent & Tool Integration
- [ ] Implement **Crawl4AI** for web research (DuckDuckGo integration).
- [ ] Build **YouTube Research Agent** with transcription (Whisper).
- [ ] Create **Document Parser** (PDF/DOCX/TXT support).
- [ ] Develop **Image Metadata Agent**.
- [ ] Set up **Email Handler** (Trace integration).
- [ ] Configure **LangChain/LlamaIndex/LangFlow** orchestration.

### Phase 3: User Interface & Export System
- [ ] Build React frontend with Vite + Shadcn UI.
- [ ] Implement real-time progress tracking and streaming.
- [ ] Create **Export System** (PDF, Word, CSV, Excel).
- [ ] Build **Research History Manager** UI.
- [ ] Add **RSPY Toggle** configuration panel.


---

## ğŸ“ Project Structure

Unlike the earlier monolithic version where all logic lived in 2 files, this architecture emphasizes clean separation of concerns.

Deep-Researcher/
â”‚
â”œâ”€â”€ backend/                    # Python FastAPI Backend (BFF Pattern)
â”‚   â”œâ”€â”€ main/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI app initialization (Gevent)
â”‚   â”‚   â”œâ”€â”€ config.py           # Environment & RSPY Toggle settings
â”‚   â”‚   â””â”€â”€ routes/             # API endpoints
â”‚   â”‚       â”œâ”€â”€ research.py     # Research task endpoints
â”‚   â”‚       â”œâ”€â”€ upload.py       # File upload handling
â”‚   â”‚       â”œâ”€â”€ export.py       # Export API (PDF/Word/CSV/Excel)
â”‚   â”‚       â””â”€â”€ health.py       # Health checks
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/                 # Specialized Research Agents
â”‚   â”‚   â”œâ”€â”€ web_research.py     # Crawl4AI-based web agent
â”‚   â”‚   â”œâ”€â”€ youtube_agent.py    # Video transcription agent
â”‚   â”‚   â”œâ”€â”€ document_agent.py   # PDF/DOCX/TXT parser
â”‚   â”‚   â”œâ”€â”€ image_metadata.py   # Image analysis agent
â”‚   â”‚   â””â”€â”€ orchestrator.py     # LangChain/LlamaIndex/LangFlow coordinator
â”‚   â”‚
â”‚   â”œâ”€â”€ tools/                  # External integrations
â”‚   â”‚   â”œâ”€â”€ crawl4ai_client.py  # Crawl4AI wrapper (DuckDuckGo)
â”‚   â”‚   â”œâ”€â”€ whisper_client.py   # Audio transcription (Whisper)
â”‚   â”‚   â”œâ”€â”€ trace_email.py      # Email ingestion
â”‚   â”‚   â””â”€â”€ youtube_api.py      # YouTube data extraction
â”‚   â”‚
â”‚   â”œâ”€â”€ parsers/                # Document processors
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py       # PDF extraction
â”‚   â”‚   â”œâ”€â”€ docx_parser.py      # Word document parsing
â”‚   â”‚   â””â”€â”€ text_parser.py      # Plain text ingestion
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                # Database & file system managers
â”‚   â”‚   â”œâ”€â”€ databases/          # SQLite database schemas
â”‚   â”‚   â”‚   â”œâ”€â”€ brain.db        # Core knowledge
â”‚   â”‚   â”‚   â”œâ”€â”€ history.db      # Session logs
â”‚   â”‚   â”‚   â”œâ”€â”€ scrapes.db      # Web scrape cache
â”‚   â”‚   â”‚   â”œâ”€â”€ research.db     # Completed reports
â”‚   â”‚   â”‚   â”œâ”€â”€ assets.db       # Media metadata
â”‚   â”‚   â”‚   â””â”€â”€ export.db       # Export jobs
â”‚   â”‚   â”œâ”€â”€ vector_store.py     # ChromaDB manager
â”‚   â”‚   â”œâ”€â”€ file_system.py      # Raw data storage handler
â”‚   â”‚   â””â”€â”€ context_builder.py  # LlamaIndex context assembly
â”‚   â”‚
â”‚   â”œâ”€â”€ export/                 # Report generation
â”‚   â”‚   â”œâ”€â”€ pdf_generator.py    # PDF export
â”‚   â”‚   â”œâ”€â”€ word_generator.py   # DOCX export
â”‚   â”‚   â”œâ”€â”€ csv_generator.py    # CSV export
â”‚   â”‚   â””â”€â”€ excel_generator.py  # XLSX export
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                    # Model abstraction layer
â”‚   â”‚   â”œâ”€â”€ ollama_client.py    # Local LLM (Ollama)
â”‚   â”‚   â”œâ”€â”€ gemini_client.py    # Google Gemini
â”‚   â”‚   â”œâ”€â”€ openai_client.py    # OpenAI GPT
â”‚   â”‚   â””â”€â”€ claude_client.py    # Anthropic Claude
â”‚   â”‚
â”‚   â”œâ”€â”€ queue/                  # Background task management
â”‚   â”‚   â”œâ”€â”€ task_queue.py       # Async job queue
â”‚   â”‚   â””â”€â”€ progress_tracker.py # Real-time progress updates
â”‚   â”‚
â”‚   â””â”€â”€ requirements.txt        # Python dependencies
â”‚
â”œâ”€â”€ app/                        # Vite + React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ AgentViewer.tsx     # Real-time agent thoughts
â”‚   â”‚   â”‚   â”œâ”€â”€ HistoryPanel.tsx    # Research history browser
â”‚   â”‚   â”‚   â”œâ”€â”€ ExportDialog.tsx    # Export format selector
â”‚   â”‚   â”‚   â”œâ”€â”€ ConfigPanel.tsx     # RSPY Toggle UI
â”‚   â”‚   â”‚   â””â”€â”€ SourcePanel.tsx     # Citations & evidence
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ hooks/              # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ useResearch.ts      # API integration
â”‚   â”‚   â”‚   â”œâ”€â”€ useStreaming.ts     # SSE/WebSocket
â”‚   â”‚   â”‚   â””â”€â”€ useProgress.ts      # Progress tracking
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ lib/                # Utilities
â”‚   â”‚       â”œâ”€â”€ api.ts              # Backend client
â”‚   â”‚       â”œâ”€â”€ markdown.ts         # Rendering utilities
â”‚   â”‚       â””â”€â”€ export.ts           # Export helpers
â”‚   â”‚
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md                   # This file
```

### Key Organization Principles
1. **Backend Modularity**: Each agent, tool, and storage layer has its own module.
2. **Storage Separation**: 6 SQLite databases for different data types + ChromaDB for vectors + file system for raw data.
3. **Export System**: Dedicated module for multi-format report generation.
4. **BFF Pattern**: FastAPI acts as a backend-for-frontend with specialized endpoints.
5. **Frontend Component Isolation**: UI components are separated from business logic (hooks).
6. **Configuration Management**: All secrets, API keys, and RSPY toggles in `.env` files.
7. **Testability**: Each module can be unit-tested independently.

---

## ğŸš€ Getting Started

1. **Prerequisites**: Python 3.12+, Node.js 20+, Ollama, Redis.
2. **Setup**:
   ```bash
   # Backend
   cd backend
   pip install -r requirements.txt
   
   # Frontend
   cd app
   npm install
   npm run dev
   ```
